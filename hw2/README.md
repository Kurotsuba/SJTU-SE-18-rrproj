# 计算机硬件

## 目录
- [CPU](#CPU)(蔡忠玮)
- [Memory](#Memory)(冯歆骅)
- [Storage](#Storage)((励颖)
- [Network](#Network)(赵樱)
- [xPU](#xPU)(王韡熙)
## CPU



## Memory

## Storage（Ceph）
##### Introduction
Ceph是一种为优秀的性能、可靠性和可扩展性而设计的统一的、分布式的存储系统

##### Feature
- 可靠性：由数据的多副本提供可靠性
- 可扩展性：可以增加元数据服务器和存储节点。容量可扩展。文件操作性能可扩展。元数据操作性能可扩展。
- Metadata Server：多个MDS，不存在单点故障和瓶颈；
- 数据分布：文件被分片，每个数据块是一个对象。对象保存在不同的存储服务器上；
- 故障恢复：当节点失效时，自动迁移数据、重新复制副本。


##### Implement
![ceph-系统结构](/image/ceph-系统架构.png)
- **解耦数据和元数据操作**
    通过消除文件分配表并将其替换为生成函数来解耦，这样就允许ceph利用OSD的智能来分布围绕数据访问、更新序列化、复制和可靠性、故障检测和恢复的复杂性
<br/>
- **动态分布式元数据管理**
    ceph利用基于动态子树分区的新型元数据集群架构，自适应地智能分配管理文件系统目录的责任，提高工作负载的性能
    ![ceph-子树结构](/image/ceph-子树结构.png)
<br/>
- **可靠的自主分布式对象存储**
    ceph将数据迁移，复制，故障检测和故障恢复以分布式方式委派给OSD，实现了容量和聚合性能的线性扩展
    ![ceph-并行](/image/ceph-并行.png)
<br/>
- **CRUSH算法**
    crush考虑了容灾域的隔离，能够实现各类负载的副本放置规则，还支持副本和EC两种数据冗余方式，提供了四种不同类型的Bucket（Uniform, List, Tree, Straw），充分考虑了实际生产过程中硬件的迭代式部署方式
    ![ceph-crush](/image/ceph-crush.jpg)


##### Pros
- **功能强大**：支持数千节点，支持动态增加节点，自动平衡数据分布
- **可配置性强**：可以针对不同场景调优
- **特性丰富**：滚动升级、多存储池、延迟删除、快照、纠删码、跨存储池缓存等
- **开源系统**：免费，初始成本低

##### Cons
- **不稳定**：目前还在实验阶段，不适合于生产环境。
- **开源系统**：后期运维成本高
- **系统复杂**：性能不高，优化难度和成本高
- **配置优化**：部署配置优化复杂，对运维能力要求非常高

##### Key Indicators
- OSD吞吐量
![ceph-p5](/image/ceph-p5.jpg)
通过观察，性能受到原始磁盘带宽（大约58MB/S）的限制
![ceph-p6](/image/ceph-p6.jpg)
通过比较，EBOFS中的小读取和写入性能受到粗线程和锁定的影响，但对于大于32KB的写入大小，显示出优于其他读取工作负载，因为数据在磁盘上的范围内布局匹配写入大小
<br/>
- 写延迟
![ceph-p7](/image/ceph-p7.jpg)
通过观察，两个以上的副本会导致小写入的最小额外成本，对于大型同步写入，传输时间占主导地位
<br/>
- 数据分布和可扩展性
![ceph-p8](/image/ceph-p8.jpg)
通过观察，OSD写入性能与OSD群集的大小成线性比例，直到切换为止，在24个OSD处饱和。当更多PG降低OSD利用率的差异时，CRUSH和散列性能会提高
<br/>
- 元数据更新延迟
![ceph-p9](/image/ceph-p9.png)
通过观察，使用本地磁盘可以避免初始网络往返，从而降低写入延迟。
<br/>
- 元数据缩放
![ceph-p10](/image/ceph-p10.jpg)
随着集群增长到128个节点，对于大多数工作负载而言，效率下降不超过完美线性扩展的50%，大大提高了现有系统的性能、

##### Comment
对于一个存储系统来说，我们无疑关注几个点：数据安全、可线性扩展、可靠性、性能。而Ceph正是针对这几个特性进行设计的，并且在性能、可扩展性、可靠性上表现较出色，可以算是发展得较为成熟的开源存储系统，但是它仍有许多问题，比如代码质量、系统复杂性等，如果可以继续改进，那它势必将大放光彩。


## Network

## xPU